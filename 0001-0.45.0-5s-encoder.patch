From 73e20b4fcfca5cb1be11b02346c9b43bc8e3d07a Mon Sep 17 00:00:00 2001
From: "junehyung.kim" <junehyung007@gmail.com>
Date: Wed, 11 Feb 2026 23:36:27 +0900
Subject: [PATCH 1/2] [0.45.0] 5s encoder
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

- qai_hub_models에서 pip install -e .
- python scripts/export_whisper_small_quantized_500.py --skip-compile
- (qai_hub 로그인 후) python scripts/export_whisper_small_quantized_500.py --skip-compile
---
 requirements.txt                              | 179 ++++++++
 scripts/export_whisper_small_quantized_500.py | 434 ++++++++++++++++++
 2 files changed, 613 insertions(+)
 create mode 100644 requirements.txt
 create mode 100644 scripts/export_whisper_small_quantized_500.py

diff --git a/requirements.txt b/requirements.txt
new file mode 100644
index 00000000..43a9ebfa
--- /dev/null
+++ b/requirements.txt
@@ -0,0 +1,179 @@
+ai-edge-litert==2.1.2
+aimet-onnx==2.24.0
+aiohappyeyeballs==2.6.1
+aiohttp==3.13.3
+aiosignal==1.4.0
+alabaster==1.0.0
+annotated-doc==0.0.4
+annotated-types==0.7.0
+anyio==4.12.1
+asttokens==3.0.1
+async-timeout==5.0.1
+attrs==25.4.0
+babel==2.18.0
+backcall==0.2.0
+backoff==2.2.1
+backports.strenum==1.3.1
+beautifulsoup4==4.14.3
+bleach==6.3.0
+bokeh==3.8.2
+botocore==1.42.46
+certifi==2026.1.4
+cffi==2.0.0
+charset-normalizer==3.4.4
+click==8.3.1
+colorcet==3.1.0
+coloredlogs==15.0.1
+contextlib2==21.6.0
+contourpy==1.3.2
+coverage==7.13.4
+datasets==4.1.1
+decorator==5.2.1
+deprecation==2.1.0
+dill==0.4.0
+docutils==0.21.2
+exceptiongroup==1.3.1
+executing==2.2.1
+filelock==3.20.3
+flatbuffers==25.12.19
+frozenlist==1.8.0
+fsspec==2025.9.0
+gdown==4.7.1
+gitdb==4.0.12
+GitPython==3.1.42
+h11==0.16.0
+h5py==3.15.1
+hf-xet==1.2.0
+holoviews==1.22.1
+httpcore==1.0.9
+httpx==0.28.1
+huggingface_hub==1.4.1
+humanfriendly==10.0
+hvplot==0.12.2
+idna==3.11
+imagesize==1.4.1
+ipython==8.12.3
+jedi==0.19.2
+Jinja2==3.1.6
+jmespath==1.1.0
+joblib==1.5.3
+jsonschema==4.26.0
+jsonschema-specifications==2025.9.1
+linkify-it-py==2.0.3
+Markdown==3.10.2
+markdown-it-py==4.0.0
+MarkupSafe==3.0.3
+matplotlib-inline==0.2.1
+mdit-py-plugins==0.5.0
+mdurl==0.1.2
+mpmath==1.3.0
+multidict==6.7.1
+multiprocess==0.70.16
+narwhals==2.16.0
+networkx==3.4.2
+numpy==2.2.6
+numpydoc==1.9.0
+nvidia-cublas-cu12==12.8.4.1
+nvidia-cuda-cupti-cu12==12.8.90
+nvidia-cuda-nvrtc-cu12==12.8.93
+nvidia-cuda-runtime-cu12==12.8.90
+nvidia-cudnn-cu12==9.10.2.21
+nvidia-cufft-cu12==11.3.3.83
+nvidia-cufile-cu12==1.13.1.3
+nvidia-curand-cu12==10.3.9.90
+nvidia-cusolver-cu12==11.7.3.90
+nvidia-cusparse-cu12==12.5.8.93
+nvidia-cusparselt-cu12==0.7.1
+nvidia-nccl-cu12==2.27.5
+nvidia-nvjitlink-cu12==12.8.93
+nvidia-nvshmem-cu12==3.3.20
+nvidia-nvtx-cu12==12.8.90
+onnx==1.18.0
+onnx2torch==1.5.15
+onnxruntime==1.22.1
+onnxsim==0.4.36
+opencv-python==4.13.0.92
+osqp==1.1.1
+packaging==26.0
+pandas==2.2.3
+panel==1.8.7
+param==2.3.2
+parso==0.8.6
+pexpect==4.9.0
+pickleshare==0.7.5
+pillow==11.3.0
+pip==22.0.2
+prettytable==3.11.0
+prompt_toolkit==3.0.52
+propcache==0.4.1
+protobuf==6.31.1
+psutil==7.2.2
+ptyprocess==0.7.0
+pure_eval==0.2.3
+pyarrow==21.0.0
+pybind11==3.0.1
+pycparser==3.0
+pydantic==2.12.5
+pydantic_core==2.41.5
+pydantic_yaml==1.4.0
+Pygments==2.19.2
+PySocks==1.7.1
+python-dateutil==2.9.0.post0
+pytz==2025.2
+pyviz_comms==3.0.6
+PyYAML==6.0.3
+qai-hub==0.44.0
+qai_hub_models==0.45.0
+referencing==0.37.0
+regex==2026.1.15
+requests==2.32.5
+requests-toolbelt==1.0.0
+rich==14.3.2
+rpds-py==0.30.0
+ruamel.yaml==0.18.10
+ruamel.yaml.clib==0.2.15
+s3transfer==0.13.1
+safetensors==0.7.0
+schema==0.7.5
+scikit-learn==1.7.2
+scipy==1.15.3
+semver==3.0.4
+setuptools==59.6.0
+shellingham==1.5.4
+six==1.17.0
+smmap==5.0.2
+snowballstemmer==3.0.1
+sounddevice==0.5.5
+soupsieve==2.8.3
+Sphinx==8.1.3
+sphinxcontrib-applehelp==2.0.0
+sphinxcontrib-devhelp==2.0.0
+sphinxcontrib-htmlhelp==2.1.0
+sphinxcontrib-jsmath==1.0.1
+sphinxcontrib-qthelp==2.0.0
+sphinxcontrib-serializinghtml==2.0.0
+stack-data==0.6.3
+sympy==1.14.0
+tabulate==0.9.0
+threadpoolctl==3.6.0
+tokenizers==0.22.2
+tomli==2.4.0
+torch==2.9.1
+torchvision==0.24.1
+tornado==6.5.4
+tqdm==4.67.3
+traitlets==5.14.3
+transformers==5.1.0
+triton==3.5.1
+typer==0.23.0
+typer-slim==0.23.0
+typing_extensions==4.15.0
+typing-inspection==0.4.2
+tzdata==2025.3
+uc-micro-py==1.0.3
+urllib3==2.6.3
+wcwidth==0.6.0
+webencodings==0.5.1
+xxhash==3.6.0
+xyzservices==2025.11.0
+yarl==1.22.0
diff --git a/scripts/export_whisper_small_quantized_500.py b/scripts/export_whisper_small_quantized_500.py
new file mode 100644
index 00000000..77662a7c
--- /dev/null
+++ b/scripts/export_whisper_small_quantized_500.py
@@ -0,0 +1,434 @@
+#!/usr/bin/env python3
+# ---------------------------------------------------------------------
+# Export Whisper Small Quantized with encoder input (1, 80, 500) for QCS6490.
+#
+# Changes vs. original (3000-step / 30s):
+#   - Encoder mel input:  (1, 80, 3000) -> (1, 80, 500)  (~5 s audio)
+#   - Positional embedding: (1500, 768) -> (250, 768)
+#   - Decoder cross-attn KV cache temporal dim: 1500 -> 250
+#   - Re-calibration with 5-second mel data
+#
+# Usage:
+#   python scripts/export_whisper_small_quantized_500.py \
+#       --calib-npz calib_out/calib_whisper_5s_enko.npz \
+#       --output-dir calib_out/whisper_small_q_500 \
+#       --skip-compile          # omit to submit AI Hub compile jobs
+# ---------------------------------------------------------------------
+from __future__ import annotations
+
+import argparse
+import os
+import zipfile
+
+import numpy as np
+import torch
+import torch.nn as nn
+
+# -- AIMET / ONNX imports -----------------------------------------------------
+import aimet_onnx
+from aimet_common.defs import QuantScheme
+from aimet_onnx.cross_layer_equalization import equalize_model
+from aimet_onnx.quantsim import QuantizationSimModel
+from onnxsim import simplify
+
+# -- Project imports -----------------------------------------------------------
+from qai_hub_models.models._shared.hf_whisper.model import (
+    MEAN_DECODE_LEN,
+    HfWhisperDecoder,
+    HfWhisperEncoder,
+)
+from qai_hub_models.models._shared.hf_whisper_quantized.model import (
+    WHISPER_AIMET_CONFIG,
+)
+from qai_hub_models.models.whisper_small.model import WhisperSmall
+from qai_hub_models.utils.input_spec import make_torch_inputs
+from qai_hub_models.utils.onnx.helpers import safe_torch_onnx_export
+
+# -- Constants -----------------------------------------------------------------
+NEW_MELS_AUDIO_LEN = 500  # 500 time-steps ~ 5 s audio
+NEW_AUDIO_EMB_LEN = 250  # 500 / 2 (conv2 stride=2)
+
+NUM_BLOCKS = 12
+ATTENTION_DIM = 768
+NUM_HEADS = 12
+HEAD_DIM = ATTENTION_DIM // NUM_HEADS  # 64
+
+SOT_TOKEN_ID = 50258  # <|startoftranscript|>
+MASK_NEG = -100.0
+
+DEFAULT_CALIB_NPZ = "calib_out/calib_whisper_5s_enko.npz"
+DEFAULT_OUTPUT_DIR = "calib_out/whisper_small_q_500"
+
+
+# -- Input-spec helpers --------------------------------------------------------
+# ONNX export uses float32 specs (for torch export & AIMET calibration).
+# AI Hub compile uses quantized dtype specs (matching AIMET encodings).
+
+def get_encoder_input_spec():
+    """Float32 spec for ONNX export / calibration."""
+    return dict(input_features=((1, 80, NEW_MELS_AUDIO_LEN), "float32"))
+
+
+def get_encoder_compile_spec():
+    """Quantized spec for AI Hub compile (matches AIMET encodings: INT16 unsigned)."""
+    return dict(input_features=((1, 80, NEW_MELS_AUDIO_LEN), "uint16"))
+
+
+def get_decoder_input_spec():
+    """Float32 spec for ONNX export / calibration."""
+    specs = dict(
+        input_ids=((1, 1), "int32"),
+        attention_mask=((1, 1, 1, MEAN_DECODE_LEN), "float32"),
+    )
+    for i in range(NUM_BLOCKS):
+        specs[f"k_cache_self_{i}_in"] = (
+            (NUM_HEADS, 1, HEAD_DIM, MEAN_DECODE_LEN - 1),
+            "float32",
+        )
+        specs[f"v_cache_self_{i}_in"] = (
+            (NUM_HEADS, 1, MEAN_DECODE_LEN - 1, HEAD_DIM),
+            "float32",
+        )
+    for i in range(NUM_BLOCKS):
+        specs[f"k_cache_cross_{i}"] = (
+            (NUM_HEADS, 1, HEAD_DIM, NEW_AUDIO_EMB_LEN),
+            "float32",
+        )
+        specs[f"v_cache_cross_{i}"] = (
+            (NUM_HEADS, 1, NEW_AUDIO_EMB_LEN, HEAD_DIM),
+            "float32",
+        )
+    specs["position_ids"] = ((1,), "int32")
+    return specs
+
+
+def get_decoder_compile_spec():
+    """Quantized spec for AI Hub compile (matches AIMET encodings)."""
+    specs = dict(
+        input_ids=((1, 1), "int32"),
+        attention_mask=((1, 1, 1, MEAN_DECODE_LEN), "uint16"),
+    )
+    for i in range(NUM_BLOCKS):
+        specs[f"k_cache_self_{i}_in"] = (
+            (NUM_HEADS, 1, HEAD_DIM, MEAN_DECODE_LEN - 1),
+            "uint16",
+        )
+        specs[f"v_cache_self_{i}_in"] = (
+            (NUM_HEADS, 1, MEAN_DECODE_LEN - 1, HEAD_DIM),
+            "uint16",
+        )
+    for i in range(NUM_BLOCKS):
+        specs[f"k_cache_cross_{i}"] = (
+            (NUM_HEADS, 1, HEAD_DIM, NEW_AUDIO_EMB_LEN),
+            "uint16",
+        )
+        specs[f"v_cache_cross_{i}"] = (
+            (NUM_HEADS, 1, NEW_AUDIO_EMB_LEN, HEAD_DIM),
+            "uint16",
+        )
+    specs["position_ids"] = ((1,), "int32")
+    return specs
+
+
+# -- Encoder: ONNX export + AIMET quantize ------------------------------------
+def export_and_quantize_encoder(fp_encoder, calib_mels, output_dir):
+    """
+    Export encoder to ONNX, simplify, apply CLE, create QuantSim,
+    calibrate with mel data, and export AIMET encodings.
+    """
+    print("\n========== Encoder: ONNX export + quantize ==========")
+    enc_dir = os.path.join(output_dir, "encoder_aimet")
+    os.makedirs(enc_dir, exist_ok=True)
+
+    # -- ONNX export --
+    enc_specs = get_encoder_input_spec()
+    dummy_input = tuple(make_torch_inputs(enc_specs))
+    raw_onnx = os.path.join(enc_dir, "encoder_raw.onnx")
+    output_names = HfWhisperEncoder.get_output_names(NUM_BLOCKS)
+
+    fp_encoder.eval()
+    safe_torch_onnx_export(
+        fp_encoder,
+        dummy_input,
+        raw_onnx,
+        export_params=True,
+        input_names=list(enc_specs.keys()),
+        output_names=output_names,
+    )
+    print(f"  Raw ONNX -> {raw_onnx}")
+
+    # -- Simplify + CLE --
+    print("  Simplifying ONNX ...")
+    onnx_model, _ = simplify(raw_onnx)
+    print("  Cross-layer equalization ...")
+    equalize_model(onnx_model)
+
+    # -- QuantSim (w8a16, min-max, per-tensor) --
+    print("  Creating QuantizationSimModel (w8a16) ...")
+    quant_sim = QuantizationSimModel(
+        model=onnx_model,
+        quant_scheme=QuantScheme.min_max,
+        param_type=aimet_onnx.int8,
+        activation_type=aimet_onnx.int16,
+        config_file=WHISPER_AIMET_CONFIG,
+    )
+
+    # -- Calibrate --
+    n = len(calib_mels)
+    print(f"  Calibrating encoder with {n} samples ...")
+    input_names = [inp.name for inp in quant_sim.session.get_inputs()]
+    calib_data = [{input_names[0]: calib_mels[i]} for i in range(n)]
+    quant_sim.compute_encodings(calib_data)
+
+    # -- Export AIMET model --
+    quant_sim.export(enc_dir, "model")
+    print(f"  AIMET exported -> {enc_dir}/model.onnx + model.encodings")
+
+    # Clean up intermediate file
+    if os.path.exists(raw_onnx):
+        os.remove(raw_onnx)
+
+    return quant_sim
+
+
+# -- Decoder: ONNX export + AIMET quantize ------------------------------------
+def export_and_quantize_decoder(
+    fp_encoder, fp_decoder, calib_mels, output_dir, num_decoder_calib=20
+):
+    """
+    Export decoder to ONNX, simplify, CLE, QuantSim, calibrate using
+    encoder-generated cross-attn KV caches, and export AIMET encodings.
+    """
+    print("\n========== Decoder: ONNX export + quantize ==========")
+    dec_dir = os.path.join(output_dir, "decoder_aimet")
+    os.makedirs(dec_dir, exist_ok=True)
+
+    # -- ONNX export --
+    dec_specs = get_decoder_input_spec()
+    dummy_input = tuple(make_torch_inputs(dec_specs))
+    raw_onnx = os.path.join(dec_dir, "decoder_raw.onnx")
+    output_names = HfWhisperDecoder.get_output_names(NUM_BLOCKS)
+
+    fp_decoder.eval()
+    safe_torch_onnx_export(
+        fp_decoder,
+        dummy_input,
+        raw_onnx,
+        export_params=True,
+        input_names=list(dec_specs.keys()),
+        output_names=output_names,
+    )
+    print(f"  Raw ONNX -> {raw_onnx}")
+
+    # -- Simplify + CLE --
+    print("  Simplifying ONNX ...")
+    onnx_model, _ = simplify(raw_onnx)
+    print("  Cross-layer equalization ...")
+    equalize_model(onnx_model)
+
+    # -- QuantSim --
+    print("  Creating QuantizationSimModel (w8a16) ...")
+    quant_sim = QuantizationSimModel(
+        model=onnx_model,
+        quant_scheme=QuantScheme.min_max,
+        param_type=aimet_onnx.int8,
+        activation_type=aimet_onnx.int16,
+        config_file=WHISPER_AIMET_CONFIG,
+    )
+
+    # -- Build decoder calibration data via fp encoder --
+    n = min(num_decoder_calib, len(calib_mels))
+    print(f"  Generating decoder calibration data ({n} samples via fp encoder) ...")
+    input_names = [inp.name for inp in quant_sim.session.get_inputs()]
+
+    calib_data = []
+    fp_encoder.eval()
+    with torch.no_grad():
+        for i in range(n):
+            mel_tensor = torch.from_numpy(calib_mels[i])  # (1, 80, 500)
+            # encoder_out: tuple of (k, v) per decoder layer
+            #   k: (NUM_HEADS, 1, HEAD_DIM, 250)
+            #   v: (NUM_HEADS, 1, 250, HEAD_DIM)
+            encoder_out = fp_encoder(mel_tensor)
+
+            sample: dict[str, np.ndarray] = {}
+
+            # Token: SOT
+            sample["input_ids"] = np.array([[SOT_TOKEN_ID]], dtype=np.int32)
+
+            # Causal attention mask: first decoding step -> attend only to
+            # the last position (current token appended after 199-len KV cache)
+            attn_mask = np.full(
+                (1, 1, 1, MEAN_DECODE_LEN), MASK_NEG, dtype=np.float32
+            )
+            attn_mask[0, 0, 0, MEAN_DECODE_LEN - 1] = 0.0
+            sample["attention_mask"] = attn_mask
+
+            # Self-attn KV cache: zeros (no previously decoded tokens)
+            for j in range(NUM_BLOCKS):
+                sample[f"k_cache_self_{j}_in"] = np.zeros(
+                    (NUM_HEADS, 1, HEAD_DIM, MEAN_DECODE_LEN - 1), dtype=np.float32
+                )
+                sample[f"v_cache_self_{j}_in"] = np.zeros(
+                    (NUM_HEADS, 1, MEAN_DECODE_LEN - 1, HEAD_DIM), dtype=np.float32
+                )
+
+            # Cross-attn KV cache from encoder output
+            for j in range(NUM_BLOCKS):
+                sample[f"k_cache_cross_{j}"] = encoder_out[j][0].cpu().numpy()
+                sample[f"v_cache_cross_{j}"] = encoder_out[j][1].cpu().numpy()
+
+            sample["position_ids"] = np.array([0], dtype=np.int32)
+
+            # Reorder to match ONNX session input order
+            ordered = {name: sample[name] for name in input_names}
+            calib_data.append(ordered)
+
+    # -- Calibrate --
+    print(f"  Calibrating decoder with {len(calib_data)} samples ...")
+    quant_sim.compute_encodings(calib_data)
+
+    # -- Export AIMET model --
+    quant_sim.export(dec_dir, "model")
+    print(f"  AIMET exported -> {dec_dir}/model.onnx + model.encodings")
+
+    if os.path.exists(raw_onnx):
+        os.remove(raw_onnx)
+
+    return quant_sim
+
+
+# -- AI Hub compile ------------------------------------------------------------
+def compile_on_hub(output_dir):
+    """Submit compile jobs to Qualcomm AI Hub for QCS6490."""
+    print("\n========== Compile on AI Hub (QCS6490) ==========")
+    try:
+        import qai_hub as hub
+    except ImportError:
+        print("  qai_hub not installed - skipping compilation.")
+        return
+
+    device = hub.Device("Dragonwing RB3 Gen 2 Vision Kit")
+
+    for component, spec_fn in [
+        ("encoder_aimet", get_encoder_input_spec),
+        ("decoder_aimet", get_decoder_input_spec),
+    ]:
+        aimet_dir = os.path.join(output_dir, component)
+        onnx_file = os.path.join(aimet_dir, "model.onnx")
+        enc_file = os.path.join(aimet_dir, "model.encodings")
+
+        if not os.path.exists(onnx_file):
+            print(f"  {onnx_file} not found - skipping {component}")
+            continue
+
+        # Create AIMET zip expected by AI Hub
+        zip_path = os.path.join(output_dir, f"{component}.aimet.zip")
+        base_dir = f"{component}.aimet"
+        with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zf:
+            zf.write(onnx_file, f"{base_dir}/model.onnx")
+            if os.path.exists(enc_file):
+                zf.write(enc_file, f"{base_dir}/model.encodings")
+            data_file = os.path.join(aimet_dir, "model.data")
+            if os.path.exists(data_file):
+                zf.write(data_file, f"{base_dir}/model.data")
+
+        input_spec = spec_fn()
+        job_name = f"whisper_small_500_{component.replace('_aimet', '')}"
+
+        print(f"  Submitting compile job: {component} ...")
+        compile_job = hub.submit_compile_job(
+            model=zip_path,
+            input_specs=input_spec,
+            device=device,
+            name=job_name,
+            options="--target_runtime precompiled_qnn_onnx --qairt_version 2.37 --quantize_io",
+        )
+        print(f"    -> {compile_job.url}")
+
+
+# -- Main ----------------------------------------------------------------------
+def main():
+    parser = argparse.ArgumentParser(
+        description="Export Whisper Small Quantized (500-mel / ~5s) for QCS6490"
+    )
+    parser.add_argument(
+        "--calib-npz",
+        default=DEFAULT_CALIB_NPZ,
+        help="Calibration NPZ with 'mels' key, shape (N, 1, 80, 500) float32",
+    )
+    parser.add_argument(
+        "--output-dir",
+        default=DEFAULT_OUTPUT_DIR,
+        help="Output directory for AIMET models",
+    )
+    parser.add_argument(
+        "--decoder-calib-samples",
+        type=int,
+        default=20,
+        help="Number of mel samples used for decoder calibration",
+    )
+    parser.add_argument(
+        "--skip-compile",
+        action="store_true",
+        help="Skip the AI Hub compilation step",
+    )
+    parser.add_argument(
+        "--compile-only",
+        action="store_true",
+        help="Skip export/quantize; submit existing ONNX + encodings to AI Hub",
+    )
+    args = parser.parse_args()
+
+    if args.compile_only and args.skip_compile:
+        parser.error("--compile-only and --skip-compile are mutually exclusive")
+
+    # ---- Compile-only: reuse existing AIMET outputs ----
+    if args.compile_only:
+        compile_on_hub(args.output_dir)
+    else:
+        # ---- Load calibration data ----
+        print(f"Loading calibration data: {args.calib_npz}")
+        npz = np.load(args.calib_npz)
+        calib_mels = npz["mels"]  # (N, 1, 80, 500) float32
+        print(f"  mels shape={calib_mels.shape}  dtype={calib_mels.dtype}")
+
+        # ---- Load FP model (with monkey-patch) ----
+        print("Loading WhisperSmall FP model ...")
+        whisper = WhisperSmall.from_pretrained()
+        fp_encoder = whisper.encoder  # HfWhisperEncoder -> QcWhisperEncoder
+        fp_decoder = whisper.decoder  # HfWhisperDecoder -> QcWhisperDecoder
+
+        # ---- Trim encoder positional embedding: (1500, 768) -> (250, 768) ----
+        orig = fp_encoder.encoder.embed_positions.shape
+        fp_encoder.encoder.embed_positions = nn.Parameter(
+            fp_encoder.encoder.embed_positions[:NEW_AUDIO_EMB_LEN, :]
+        )
+        print(
+            f"  embed_positions: {orig} -> {fp_encoder.encoder.embed_positions.shape}"
+        )
+
+        # ---- Export + quantize encoder ----
+        os.makedirs(args.output_dir, exist_ok=True)
+        export_and_quantize_encoder(fp_encoder, calib_mels, args.output_dir)
+
+        # ---- Export + quantize decoder ----
+        export_and_quantize_decoder(
+            fp_encoder,
+            fp_decoder,
+            calib_mels,
+            args.output_dir,
+            num_decoder_calib=args.decoder_calib_samples,
+        )
+
+        # ---- Compile on AI Hub ----
+        if not args.skip_compile:
+            compile_on_hub(args.output_dir)
+        else:
+            print("\n  --skip-compile: AI Hub compilation skipped.")
+
+    print(f"\nDone. Output -> {args.output_dir}")
+
+
+if __name__ == "__main__":
+    main()
-- 
2.34.1

